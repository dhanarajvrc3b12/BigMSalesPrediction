# Preprocessing Pipeline

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Load data
train = pd.read_csv("../DATA/train_v9rqX0R.csv")

# Target variable
y = train['Item_Outlet_Sales']
X = train.drop(columns=['Item_Outlet_Sales'])

# Step 1: Fix inconsistent labels

X['Item_Fat_Content'] = X['Item_Fat_Content'].replace({
    'LF': 'Low Fat',
    'low fat': 'Low Fat',
    'reg': 'Regular'
})


# Step 2: Define columns

numeric_features = ['Item_Weight', 'Item_Visibility', 'Item_MRP']
categorical_features = ['Item_Fat_Content', 'Item_Type', 
                        'Outlet_Identifier', 'Outlet_Size', 
                        'Outlet_Location_Type', 'Outlet_Type']

# Step 3: Define transformers

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

# Step 4: Column transformer

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)


# Step 5: Fit + transform
X_processed = preprocessor.fit_transform(X)

# Get feature names after transformation
cat_ohe = preprocessor.named_transformers_['cat']['onehot']
cat_feature_names = cat_ohe.get_feature_names_out(categorical_features)

all_feature_names = numeric_features + list(cat_feature_names)

X_processed_df = pd.DataFrame(X_processed.toarray() if hasattr(X_processed, "toarray") else X_processed,
                              columns=all_feature_names)

print("Train shape after preprocessing:", X_processed_df.shape)




#SPlitting the data
X_train, X_valid, y_train, y_valid = train_test_split(
    X_processed_df, y, test_size=0.2, random_state=42
)


X_processed_df


# ML Pipeline with Cross Validation & Hyperparameter Tuning (using preprocessed data)

from sklearn.model_selection import KFold, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import numpy as np
import pandas as pd



# Define models with hyperparameter grids (no preprocessing here)
models = {
    "LinearRegression": {
        "model": LinearRegression(),
        "params": {}
    },
    "Ridge": {
        "model": Ridge(),
        "params": {"alpha": [0.1, 1.0, 10.0]}
    },
    "RandomForest": {
        "model": RandomForestRegressor(random_state=42),
        "params": {
            "n_estimators": [100, 200],
            "max_depth": [10, 20],
            "min_samples_split": [2, 5]
        }
    },
    "XGBoost": {
        "model": XGBRegressor(objective="reg:squarederror", random_state=42),
        "params": {
            "n_estimators": [100, 200],
            "max_depth": [3, 6],
            "learning_rate": [0.05, 0.1]
        }
    }
}

# K-Fold setup
kf = KFold(n_splits=5, shuffle=True, random_state=42)

results = []

for name, mp in models.items():
    print(f" Running GridSearchCV for {name} ...")
    grid = GridSearchCV(mp["model"], mp["params"], cv=kf,
                        scoring="neg_root_mean_squared_error",
                        n_jobs=-1, verbose=1)
    grid.fit(X_train, y_train)

    results.append({
        "Model": name,
        "Best Params": grid.best_params_,
        "Best RMSE": -grid.best_score_
    })

# Convert results to dataframe
results_df = pd.DataFrame(results)
print("\n Model Comparison Results")
display(results_df)

# Pick best model
best_model_name = results_df.loc[results_df["Best RMSE"].idxmin(), "Model"]
print(f"\n Best Model: {best_model_name}")

# Retrain best model on full training set
best_params = results_df.loc[results_df["Best RMSE"].idxmin(), "Best Params"]
final_model = models[best_model_name]["model"].set_params(**best_params)
final_model.fit(X_train, y_train)

# Predict on preprocessed test set
test_predictions = final_model.predict(test_processed)

# Save submission
submission = test.copy()
submission["Item_Outlet_Sales"] = test_predictions
submission.to_csv("submission.csv", index=False)

print("Final submission saved as 'submission.csv'")



import joblib

# Save the final trained best model
joblib.dump(final_model, "best_model.pkl")
print(" Best model saved as best_model.pkl")











